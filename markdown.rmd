---
title: "Allstate Kaggle"
author: "Team 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# libraries
library(ggplot2)
library(moments)
library(corrplot)
library(caret)

# load data
allstate_train = read.csv("allstate_train.csv")
new.all.cd.train = read.csv("new_all_cd_train.csv")

```

## Exploring the data

### Basic data analysis.

```{r, echo = F, warning = F, message = F, cache=F}

head(str(allstate_train, list.len = 1000))
head(summary(allstate_train))
head(sapply(allstate_train, sd))
```

## Histograms

Below, we can see a histogram of the loss category. As we can see, the data is heavily skewed to the right.

```{r, echo = F, warning = F, message = F}
skew1 <- skewness(allstate_train$loss)

ggplot(data = allstate_train, mapping = aes(allstate_train$loss)) + 
  geom_histogram(aes(y = ..density..), 
                 col="white", 
                 fill="steelblue", 
                 bins = 100) + 
  geom_density(color="red") +
  labs(title="Loss Histogram", x="Loss", y="Count") +
  geom_vline(aes(xintercept=mean(allstate_train$loss)),
             color="darkgreen", linetype="dashed", size=1, alpha=0.2) +
  geom_text(
    mapping = aes(x2, y2, label = text2),
    data = data.frame(x2 = mean(allstate_train$loss), y2 = .0003, text2 = "mean[loss]==3037.338"),
    color = I("darkgreen"),
    parse = TRUE,
    hjust = 0
  ) +
  geom_vline(aes(xintercept=median(allstate_train$loss)),
             color="purple", linetype="dashed", size=1, alpha=0.2) +
  geom_text(
    mapping = aes(x2, y2, label = text3),
    data = data.frame(x2 = median(allstate_train$loss)+50, y2 = .00035, text3 = "median[loss]==2115.57"),
    color = I("purple"),
    parse = TRUE,
    hjust = 0
  ) +
  geom_text(
    mapping = aes(x2, y2, label = text4),
    data = data.frame(x2 = 80000, y2 = .00005, text4 = paste0("skewness==", round(skew1, 3))),
    fontface="italic",
    parse = TRUE,
    hjust = 0
  ) 

```

Since it is hard to visualize because of outliers, we can set the x-axis to include only the first 95% of the loss observations and leave out outliers with high values.

```{r, echo = F, warning = F, message = F}
#calculate the observations through the 95th percentile
xmax <- sort(allstate_train$loss, decreasing = FALSE)[(.95*length(allstate_train$loss))]

#graph limited  to the 95th percentile
ggplot(data = allstate_train, mapping = aes(allstate_train$loss)) + 
  geom_histogram(aes(y = ..density..), 
                 col="white", 
                 fill="steelblue", 
                 bins = 100) +   geom_density(color="red") + 
  xlim(0, xmax) +
  labs(title="Loss Histogram", x="Loss", y="Count") +
  geom_vline(aes(xintercept=mean(allstate_train$loss)),
             color="darkgreen", linetype="dashed", size=1, alpha=0.2) +
  geom_text(
    mapping = aes(x2, y2, label = text2),
    data = data.frame(x2 = mean(allstate_train$loss)+50, y2 = .0003, text2 = "mean[loss]==3037.338"),
    color = I("darkgreen"),
    parse = TRUE,
    hjust = 0
  ) +
  geom_vline(aes(xintercept=median(allstate_train$loss)),
             color="purple", linetype="dashed", size=1, alpha=0.2) +
  geom_text(
    mapping = aes(x2, y2, label = text3),
    data = data.frame(x2 = median(allstate_train$loss)+50, y2 = .00035, text3 = "median[loss]==2115.57"),
    color = I("purple"),
    parse = TRUE,
    hjust = 0
  ) +
  geom_text(
    mapping = aes(x2, y2, label = text4),
    data = data.frame(x2 = 6000, y2 = .0001, text4 = paste0("skewness==", round(skew1, 3))),
    fontface="italic",
    parse = TRUE,
    hjust = 0
  )  
```

In order to see a more normal distribution, we can set the x-axis to the log 10 of x.

```{r, echo = F, warning = F, message = F}
skew2 <- skewness(log(allstate_train$loss))

ggplot(data = allstate_train, mapping = aes(allstate_train$loss)) + 
  geom_histogram(aes(y = ..density..),
                 col = "white",
                 fill = "steelblue",
                 bins = 50) + 
  geom_density(color="red") + 
  scale_x_log10() +
  labs(title="log(Loss) Histogram", x="log(Loss)", y="")
```

There are still some outliers, as can be seen when we increase the number of bins.


```{r, echo = F, warning = F, message = F}

ggplot(data = allstate_train, mapping = aes(allstate_train$loss)) + 
  geom_histogram(aes(y = ..density..),
                 fill = "steelblue",
                 bins = 188318) + 
  geom_density(color="red") + 
  scale_x_log10() +
  labs(title="log(Loss) Histogram", x="log(Loss)", y="") +
  ylim(0, 2)
```

The log(x+200) is better(???) (credit to kaggle forums)

```{r, echo = F, warning = F, message = F}
skew3 <- skewness(log(allstate_train$loss+200))

ggplot(data = allstate_train, mapping = aes(log(allstate_train$loss+200))) + 
  geom_histogram(aes(y = ..density..),
                 fill = "steelblue",
                 bins = 188318) + 
  geom_density(color="red") + 
  scale_x_log10() +
  labs(title="log(Loss) Histogram", x="log(Loss)", y="") +
  ylim(0, 10)

ggplot(data = allstate_train, mapping = aes(log(allstate_train$loss+200))) + 
  geom_histogram(aes(y = ..density..),
                 col = "white",
                 fill = "steelblue",
                 bins = 50) + 
  geom_density(color="red") + 
  scale_x_log10() +
  labs(title="log(Loss+200) Histogram", x="log(Loss)", y="")

```

## Preprocessing the data

Because the levels within the categorical variables, we will preprocess the data and create dummy columns for each level with values of 0 or 1. In order to reduce the number of new columns, we will limit the dummy columns to categories that comprise at leas 5% of the variable. Additionally, we will account for variables that appear in the test.csv dataset, but not the train.csv dataset. Because the log(loss+200) has less skewedness (???), we will also convert the loss column to log(loss+200).

```{r, echo = F, warning = F, message = F}
str(new.all.cd.train, list.len = 250)
dim(new.all.cd.train)
```

With all the columns converted to numeric, we can calculate which columns are most correlated to the loss column.

```{r, echo = F, warning = F, message = F}
correlation_tot = sapply(new.all.cd.train[, -c(1,206)], function(x)
  cor(x, new.all.cd.train[, 206]))
correlation_tot = data.frame(labels(correlation_tot), unname(correlation_tot))
colnames(correlation_tot) = c("variable", "cor")
correlation_tot_top = head(correlation_tot[order(abs(correlation_tot$cor),decreasing = TRUE),], 20)

#boxplot(new.all.cd.train$cat80.D, new.all.cd.train$loss)
ggplot(correlation_tot_top, aes(reorder(variable, abs(cor)), cor)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Correlation of Each Column To Loss\n(sorted by absolute value, top 20)",
       x = "", y = "Correlation") +
  coord_flip(ylim = c(-0.5, 0.5)) #+
  #theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

We can also visualize the correlation between different variables:
???(need something better)

```{r, echo = F, warning = F, message = F}

corrplot(cor(new.all.cd.train[1:206]), method="circle", order ="hclust")

```


##Linear Regression

As our first model, we will run a multivariable linear regression. First we will use all variables.

```{r, echo = F, warning = F, message = F}

set.seed(0)
inTrain1<- createDataPartition(y=new.all.cd.train$loss, p=0.80, list=FALSE, times=1)
training<-new.all.cd.train[inTrain1,]
testing<-new.all.cd.train[-inTrain1,]
lmFit1<-train(loss~., data=training, method='lm')
summary(lmFit1)

```
Residual standard error: 0.51 on 150457 degrees of freedom
Multiple R-squared:  0.5152,	Adjusted R-squared:  0.5146 
F-statistic: 807.5 on 198 and 150457 DF,  p-value: < 2.2e-16

However, there are a few NA's in the model. Removing all variables with NA coefficients in model:

```{r, echo = F, warning = F, message = F}

lmFit1adj2 <- train(loss~. - cat114.OTHER -cat111.OTHER -cat103.OTHER -cat101.OTHER
                    -cat102.OTHER -cat90.OTHER -cat89.OTHER, data=training, method='lm')
summary(lmFit1adj2)
```
Residual standard error: 0.51 on 150457 degrees of freedom
Multiple R-squared:  0.5152,	Adjusted R-squared:  0.5146 
F-statistic: 807.5 on 198 and 150457 DF,  p-value: < 2.2e-16

Lastly, we will only use variables with 
```{r, echo = F, warning = F, message = F}
lmFit1adj3 <- train(loss~. - cat114.OTHER -cat111.OTHER -cat103.OTHER -cat101.OTHER
                    -cat102.OTHER -cat90.OTHER -cat89.OTHER -cat6.B -cat8.B -cat10.B
                    -cat10.B -cat15.B -cat19.B -cat19.B -cat24.B -cat30.B -cat33.B 
                    -cat43.B -cat45.B -cat46.B -cat58.B -cat60.B -cat62.B -cat64.B
                    -cat66.B -cat68.B -cat69.B -cat70.B -cat81.OTHER -cat82.B -cat82.B
                    -cat83.B -cat84.OTHER -cat86.D -cat88.D -cat88.OTHER -cat92.OTHER
                    -cat96.OTHER -cat97.C -cat97.E -cat97.OTHER -cat98.C -cat98.D
                    -cat98.OTHER -cat99.R -cat99.T -cat100.I -cat104.F -cat104.G -cat104.H 
                    -cat104.K -cat104.OTHER -cat105.E -cat105.F -cat105.H -cat106.F
                    -cat106.G -cat106.J -cat107.H -cat108.F -cat108.G -cat108.G -cat109.BI
                    -cat109.OTHER -cat110.CL -cat110.CO -cat110.EG -cat110.OTHER -cat113.AX
                    -cat113.OTHER -cat115.K -cat115.L -cat115.L -cat115.M -cat115.N -cat115.N
                    -cat115.O -cat115.OTHER -cat115.P -cont3 -cont5 -cont6 -cont13, 
                    data=training, method='lm')
summary(lmFit1adj3)


```

